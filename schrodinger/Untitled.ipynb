{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as mp\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 128\n",
    "seedmax = 20 # opens seed files 0 - 19. Lost too much data due to kernel crashes, so these got broken up\n",
    "trainx = []\n",
    "trainy = []\n",
    "validx = []\n",
    "validy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is not a ... pythonic [barf]... way of reading data, but python is stupid about pointers, so deal with it\n",
    "for i in range(seedmax):\n",
    "    with open('test_pots'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            trainx.append([float(num) for num in row])\n",
    "    with open('test_out'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            trainy.append([float(num) for num in row])\n",
    "    with open('valid_pots'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            validx.append([float(num) for num in row])\n",
    "    with open('valid_out'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            validy.append([float(num) for num in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(127, input_dim=127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = np.array(trainx)\n",
    "trainy = np.array(trainy)\n",
    "validx = np.array(validx)\n",
    "validy = np.array(validy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oscar/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100000\n",
      " - 2s - loss: 1.5643 - accuracy: 0.0151\n",
      "Epoch 2/100000\n",
      " - 1s - loss: 0.2247 - accuracy: 0.0229\n",
      "Epoch 3/100000\n",
      " - 1s - loss: 0.1453 - accuracy: 0.0317\n",
      "Epoch 4/100000\n",
      " - 1s - loss: 0.1101 - accuracy: 0.0378\n",
      "Epoch 5/100000\n",
      " - 1s - loss: 0.0859 - accuracy: 0.0457\n",
      "Epoch 6/100000\n",
      " - 1s - loss: 0.0691 - accuracy: 0.0479\n",
      "Epoch 7/100000\n",
      " - 1s - loss: 0.0594 - accuracy: 0.0532\n",
      "Epoch 8/100000\n",
      " - 1s - loss: 0.0500 - accuracy: 0.0594\n",
      "Epoch 9/100000\n",
      " - 1s - loss: 0.0433 - accuracy: 0.0629\n",
      "Epoch 10/100000\n",
      " - 1s - loss: 0.0443 - accuracy: 0.0644\n",
      "Epoch 11/100000\n",
      " - 1s - loss: 0.0350 - accuracy: 0.0726\n",
      "Epoch 12/100000\n",
      " - 1s - loss: 0.0347 - accuracy: 0.0761\n",
      "Epoch 13/100000\n",
      " - 1s - loss: 0.0311 - accuracy: 0.0792\n",
      "Epoch 14/100000\n",
      " - 1s - loss: 0.0263 - accuracy: 0.0861\n",
      "Epoch 15/100000\n",
      " - 1s - loss: 0.0255 - accuracy: 0.0872\n",
      "Epoch 16/100000\n",
      " - 1s - loss: 0.0227 - accuracy: 0.0958\n",
      "Epoch 17/100000\n",
      " - 1s - loss: 0.0209 - accuracy: 0.0943\n",
      "Epoch 18/100000\n",
      " - 1s - loss: 0.0217 - accuracy: 0.0952\n",
      "Epoch 19/100000\n",
      " - 1s - loss: 0.0237 - accuracy: 0.0980\n",
      "Epoch 20/100000\n",
      " - 1s - loss: 0.0213 - accuracy: 0.1042\n",
      "Epoch 21/100000\n",
      " - 1s - loss: 0.0214 - accuracy: 0.0992\n",
      "Epoch 22/100000\n",
      " - 1s - loss: 0.0192 - accuracy: 0.1059\n",
      "Epoch 23/100000\n",
      " - 1s - loss: 0.0188 - accuracy: 0.1114\n",
      "Epoch 24/100000\n",
      " - 1s - loss: 0.0159 - accuracy: 0.1126\n",
      "Epoch 25/100000\n",
      " - 1s - loss: 0.0167 - accuracy: 0.1194\n",
      "Epoch 26/100000\n",
      " - 1s - loss: 0.0144 - accuracy: 0.1154\n",
      "Epoch 27/100000\n",
      " - 1s - loss: 0.0147 - accuracy: 0.1242\n",
      "Epoch 28/100000\n",
      " - 1s - loss: 0.0141 - accuracy: 0.1250\n",
      "Epoch 29/100000\n",
      " - 1s - loss: 0.0144 - accuracy: 0.1197\n",
      "Epoch 30/100000\n",
      " - 1s - loss: 0.0157 - accuracy: 0.1210\n",
      "Epoch 31/100000\n",
      " - 1s - loss: 0.0144 - accuracy: 0.1205\n",
      "Epoch 32/100000\n",
      " - 1s - loss: 0.0140 - accuracy: 0.1240\n",
      "Epoch 33/100000\n",
      " - 1s - loss: 0.0127 - accuracy: 0.1254\n",
      "Epoch 34/100000\n",
      " - 1s - loss: 0.0152 - accuracy: 0.1202\n",
      "Epoch 35/100000\n",
      " - 1s - loss: 0.0143 - accuracy: 0.1291\n",
      "Epoch 36/100000\n",
      " - 1s - loss: 0.0117 - accuracy: 0.1330\n",
      "Epoch 37/100000\n",
      " - 1s - loss: 0.0138 - accuracy: 0.1308\n",
      "Epoch 38/100000\n",
      " - 1s - loss: 0.0121 - accuracy: 0.1342\n",
      "Epoch 39/100000\n",
      " - 1s - loss: 0.0094 - accuracy: 0.1473\n",
      "Epoch 40/100000\n",
      " - 1s - loss: 0.0103 - accuracy: 0.1448\n",
      "Epoch 41/100000\n",
      " - 1s - loss: 0.0105 - accuracy: 0.1421\n",
      "Epoch 42/100000\n",
      " - 1s - loss: 0.0097 - accuracy: 0.1489\n",
      "Epoch 43/100000\n",
      " - 1s - loss: 0.0093 - accuracy: 0.1435\n",
      "Epoch 44/100000\n",
      " - 1s - loss: 0.0098 - accuracy: 0.1491\n",
      "Epoch 45/100000\n",
      " - 1s - loss: 0.0093 - accuracy: 0.1493\n",
      "Epoch 46/100000\n",
      " - 1s - loss: 0.0107 - accuracy: 0.1542\n",
      "Epoch 47/100000\n",
      " - 1s - loss: 0.0141 - accuracy: 0.1371\n",
      "Epoch 48/100000\n",
      " - 1s - loss: 0.0090 - accuracy: 0.1567\n",
      "Epoch 49/100000\n",
      " - 1s - loss: 0.0080 - accuracy: 0.1591\n",
      "Epoch 50/100000\n",
      " - 1s - loss: 0.0126 - accuracy: 0.1468\n",
      "Epoch 51/100000\n",
      " - 1s - loss: 0.0069 - accuracy: 0.1678\n",
      "Epoch 52/100000\n",
      " - 1s - loss: 0.0103 - accuracy: 0.1534\n",
      "Epoch 53/100000\n",
      " - 1s - loss: 0.0091 - accuracy: 0.1591\n",
      "Epoch 54/100000\n",
      " - 1s - loss: 0.0097 - accuracy: 0.1593\n",
      "Epoch 55/100000\n",
      " - 1s - loss: 0.0079 - accuracy: 0.1655\n",
      "Epoch 56/100000\n",
      " - 1s - loss: 0.0070 - accuracy: 0.1710\n",
      "Epoch 57/100000\n",
      " - 1s - loss: 0.0056 - accuracy: 0.1778\n",
      "Epoch 58/100000\n",
      " - 1s - loss: 0.0069 - accuracy: 0.1750\n",
      "Epoch 59/100000\n",
      " - 1s - loss: 0.0064 - accuracy: 0.1771\n",
      "Epoch 60/100000\n",
      " - 1s - loss: 0.0084 - accuracy: 0.1620\n",
      "Epoch 61/100000\n",
      " - 1s - loss: 0.0071 - accuracy: 0.1786\n",
      "Epoch 62/100000\n",
      " - 1s - loss: 0.0082 - accuracy: 0.1663\n",
      "Epoch 63/100000\n",
      " - 1s - loss: 0.0075 - accuracy: 0.1807\n",
      "Epoch 64/100000\n",
      " - 1s - loss: 0.0072 - accuracy: 0.1774\n",
      "Epoch 65/100000\n",
      " - 1s - loss: 0.0084 - accuracy: 0.1780\n",
      "Epoch 66/100000\n",
      " - 1s - loss: 0.0077 - accuracy: 0.1805\n",
      "Epoch 67/100000\n",
      " - 1s - loss: 0.0076 - accuracy: 0.1736\n",
      "Epoch 68/100000\n",
      " - 1s - loss: 0.0054 - accuracy: 0.1924\n",
      "Epoch 69/100000\n",
      " - 1s - loss: 0.0051 - accuracy: 0.1895\n",
      "Epoch 70/100000\n",
      " - 1s - loss: 0.0063 - accuracy: 0.1793\n",
      "Epoch 71/100000\n",
      " - 1s - loss: 0.0073 - accuracy: 0.1731\n",
      "Epoch 72/100000\n",
      " - 1s - loss: 0.0075 - accuracy: 0.1834\n",
      "Epoch 73/100000\n",
      " - 1s - loss: 0.0064 - accuracy: 0.1779\n",
      "Epoch 74/100000\n",
      " - 1s - loss: 0.0066 - accuracy: 0.1871\n",
      "Epoch 75/100000\n",
      " - 1s - loss: 0.0059 - accuracy: 0.1915\n",
      "Epoch 76/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2024\n",
      "Epoch 77/100000\n",
      " - 1s - loss: 0.0084 - accuracy: 0.1777\n",
      "Epoch 78/100000\n",
      " - 1s - loss: 0.0054 - accuracy: 0.1882\n",
      "Epoch 79/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.1986\n",
      "Epoch 80/100000\n",
      " - 1s - loss: 0.0065 - accuracy: 0.1847\n",
      "Epoch 81/100000\n",
      " - 1s - loss: 0.0165 - accuracy: 0.1406\n",
      "Epoch 82/100000\n",
      " - 1s - loss: 0.0099 - accuracy: 0.1697\n",
      "Epoch 83/100000\n",
      " - 1s - loss: 0.0070 - accuracy: 0.1868\n",
      "Epoch 84/100000\n",
      " - 1s - loss: 0.0089 - accuracy: 0.1801\n",
      "Epoch 85/100000\n",
      " - 1s - loss: 0.0070 - accuracy: 0.1823\n",
      "Epoch 86/100000\n",
      " - 1s - loss: 0.0072 - accuracy: 0.1806\n",
      "Epoch 87/100000\n",
      " - 1s - loss: 0.0075 - accuracy: 0.1853\n",
      "Epoch 88/100000\n",
      " - 1s - loss: 0.0064 - accuracy: 0.1915\n",
      "Epoch 89/100000\n",
      " - 1s - loss: 0.0062 - accuracy: 0.1964\n",
      "Epoch 90/100000\n",
      " - 1s - loss: 0.0072 - accuracy: 0.1803\n",
      "Epoch 91/100000\n",
      " - 1s - loss: 0.0056 - accuracy: 0.2081\n",
      "Epoch 92/100000\n",
      " - 1s - loss: 0.0046 - accuracy: 0.2030\n",
      "Epoch 93/100000\n",
      " - 1s - loss: 0.0061 - accuracy: 0.2048\n",
      "Epoch 94/100000\n",
      " - 1s - loss: 0.0074 - accuracy: 0.1976\n",
      "Epoch 95/100000\n",
      " - 1s - loss: 0.0058 - accuracy: 0.2007\n",
      "Epoch 96/100000\n",
      " - 1s - loss: 0.0063 - accuracy: 0.1969\n",
      "Epoch 97/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2110\n",
      "Epoch 98/100000\n",
      " - 1s - loss: 0.0060 - accuracy: 0.1988\n",
      "Epoch 99/100000\n",
      " - 1s - loss: 0.0054 - accuracy: 0.2036\n",
      "Epoch 100/100000\n",
      " - 1s - loss: 0.0075 - accuracy: 0.1806\n",
      "Epoch 101/100000\n",
      " - 1s - loss: 0.0056 - accuracy: 0.1984\n",
      "Epoch 102/100000\n",
      " - 1s - loss: 0.0040 - accuracy: 0.2147\n",
      "Epoch 103/100000\n",
      " - 1s - loss: 0.0048 - accuracy: 0.2097\n",
      "Epoch 104/100000\n",
      " - 1s - loss: 0.0051 - accuracy: 0.2045\n",
      "Epoch 105/100000\n",
      " - 1s - loss: 0.0076 - accuracy: 0.2010\n",
      "Epoch 106/100000\n",
      " - 1s - loss: 0.0073 - accuracy: 0.2008\n",
      "Epoch 107/100000\n",
      " - 1s - loss: 0.0050 - accuracy: 0.2124\n",
      "Epoch 108/100000\n",
      " - 1s - loss: 0.0039 - accuracy: 0.2313\n",
      "Epoch 109/100000\n",
      " - 1s - loss: 0.0109 - accuracy: 0.1770\n",
      "Epoch 110/100000\n",
      " - 1s - loss: 0.0063 - accuracy: 0.2001\n",
      "Epoch 111/100000\n",
      " - 1s - loss: 0.0061 - accuracy: 0.2032\n",
      "Epoch 112/100000\n",
      " - 1s - loss: 0.0052 - accuracy: 0.2150\n",
      "Epoch 113/100000\n",
      " - 1s - loss: 0.0044 - accuracy: 0.2204\n",
      "Epoch 114/100000\n",
      " - 1s - loss: 0.0047 - accuracy: 0.2152\n",
      "Epoch 115/100000\n",
      " - 1s - loss: 0.0050 - accuracy: 0.2281\n",
      "Epoch 116/100000\n",
      " - 1s - loss: 0.0044 - accuracy: 0.2257\n",
      "Epoch 117/100000\n",
      " - 2s - loss: 0.0041 - accuracy: 0.2303\n",
      "Epoch 118/100000\n",
      " - 1s - loss: 0.0061 - accuracy: 0.2061\n",
      "Epoch 119/100000\n",
      " - 1s - loss: 0.0054 - accuracy: 0.2061\n",
      "Epoch 120/100000\n",
      " - 1s - loss: 0.0073 - accuracy: 0.2008\n",
      "Epoch 121/100000\n",
      " - 1s - loss: 0.0043 - accuracy: 0.2292\n",
      "Epoch 122/100000\n",
      " - 1s - loss: 0.0051 - accuracy: 0.2154\n",
      "Epoch 123/100000\n",
      " - 1s - loss: 0.0042 - accuracy: 0.2274\n",
      "Epoch 124/100000\n",
      " - 1s - loss: 0.0045 - accuracy: 0.2284\n",
      "Epoch 125/100000\n",
      " - 1s - loss: 0.0044 - accuracy: 0.2303\n",
      "Epoch 126/100000\n",
      " - 1s - loss: 0.0056 - accuracy: 0.2114\n",
      "Epoch 127/100000\n",
      " - 1s - loss: 0.0060 - accuracy: 0.2074\n",
      "Epoch 128/100000\n",
      " - 1s - loss: 0.0060 - accuracy: 0.2131\n",
      "Epoch 129/100000\n",
      " - 1s - loss: 0.0045 - accuracy: 0.2270\n",
      "Epoch 130/100000\n",
      " - 1s - loss: 0.0059 - accuracy: 0.2110\n",
      "Epoch 131/100000\n",
      " - 1s - loss: 0.0051 - accuracy: 0.2084\n",
      "Epoch 132/100000\n",
      " - 1s - loss: 0.0038 - accuracy: 0.2336\n",
      "Epoch 133/100000\n",
      " - 1s - loss: 0.0043 - accuracy: 0.2180\n",
      "Epoch 134/100000\n",
      " - 1s - loss: 0.0043 - accuracy: 0.2211\n",
      "Epoch 135/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2168\n",
      "Epoch 136/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2183\n",
      "Epoch 137/100000\n",
      " - 1s - loss: 0.0038 - accuracy: 0.2250\n",
      "Epoch 138/100000\n",
      " - 1s - loss: 0.0043 - accuracy: 0.2252\n",
      "Epoch 139/100000\n",
      " - 1s - loss: 0.0051 - accuracy: 0.2189\n",
      "Epoch 140/100000\n",
      " - 1s - loss: 0.0067 - accuracy: 0.2101\n",
      "Epoch 141/100000\n",
      " - 1s - loss: 0.0062 - accuracy: 0.1989\n",
      "Epoch 142/100000\n",
      " - 1s - loss: 0.0050 - accuracy: 0.2164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/100000\n",
      " - 1s - loss: 0.0041 - accuracy: 0.2277\n",
      "Epoch 144/100000\n",
      " - 1s - loss: 0.0064 - accuracy: 0.2061\n",
      "Epoch 145/100000\n",
      " - 1s - loss: 0.0054 - accuracy: 0.2131\n",
      "Epoch 146/100000\n",
      " - 1s - loss: 0.0041 - accuracy: 0.2250\n",
      "Epoch 147/100000\n",
      " - 1s - loss: 0.0047 - accuracy: 0.2174\n",
      "Epoch 148/100000\n",
      " - 1s - loss: 0.0037 - accuracy: 0.2399\n",
      "Epoch 149/100000\n",
      " - 1s - loss: 0.0034 - accuracy: 0.2412\n",
      "Epoch 150/100000\n",
      " - 1s - loss: 0.0051 - accuracy: 0.2146\n",
      "Epoch 151/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2319\n",
      "Epoch 152/100000\n",
      " - 1s - loss: 0.0039 - accuracy: 0.2419\n",
      "Epoch 153/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2271\n",
      "Epoch 154/100000\n",
      " - 1s - loss: 0.0036 - accuracy: 0.2344\n",
      "Epoch 155/100000\n",
      " - 1s - loss: 0.0044 - accuracy: 0.2353\n",
      "Epoch 156/100000\n",
      " - 1s - loss: 0.0035 - accuracy: 0.2390\n",
      "Epoch 157/100000\n",
      " - 1s - loss: 0.0049 - accuracy: 0.2200\n",
      "Epoch 158/100000\n",
      " - 1s - loss: 0.0038 - accuracy: 0.2340\n",
      "Epoch 159/100000\n",
      " - 1s - loss: 0.0039 - accuracy: 0.2381\n",
      "Epoch 160/100000\n",
      " - 1s - loss: 0.0034 - accuracy: 0.2402\n",
      "Epoch 161/100000\n",
      " - 1s - loss: 0.0033 - accuracy: 0.2399\n",
      "Epoch 162/100000\n",
      " - 1s - loss: 0.0040 - accuracy: 0.2409\n",
      "Epoch 163/100000\n",
      " - 1s - loss: 0.0087 - accuracy: 0.1779\n",
      "Epoch 164/100000\n",
      " - 1s - loss: 0.0055 - accuracy: 0.2194\n",
      "Epoch 165/100000\n",
      " - 1s - loss: 0.0039 - accuracy: 0.2403\n",
      "Epoch 166/100000\n",
      " - 1s - loss: 0.0050 - accuracy: 0.2101\n",
      "Epoch 167/100000\n",
      " - 1s - loss: 0.0048 - accuracy: 0.2160\n",
      "Epoch 168/100000\n",
      " - 1s - loss: 0.0042 - accuracy: 0.2347\n",
      "Epoch 169/100000\n",
      " - 1s - loss: 0.0034 - accuracy: 0.2397\n",
      "Epoch 170/100000\n",
      " - 1s - loss: 0.0047 - accuracy: 0.2289\n",
      "Epoch 171/100000\n",
      " - 1s - loss: 0.0035 - accuracy: 0.2350\n",
      "Epoch 172/100000\n",
      " - 1s - loss: 0.0045 - accuracy: 0.2347\n",
      "Epoch 173/100000\n",
      " - 1s - loss: 0.0044 - accuracy: 0.2250\n",
      "Epoch 174/100000\n",
      " - 1s - loss: 0.0037 - accuracy: 0.2374\n",
      "Epoch 175/100000\n",
      " - 1s - loss: 0.0027 - accuracy: 0.2586\n",
      "Epoch 176/100000\n",
      " - 1s - loss: 0.0042 - accuracy: 0.2277\n",
      "Epoch 177/100000\n",
      " - 1s - loss: 0.0035 - accuracy: 0.2327\n",
      "Epoch 178/100000\n",
      " - 1s - loss: 0.0035 - accuracy: 0.2386\n",
      "Epoch 179/100000\n",
      " - 1s - loss: 0.0059 - accuracy: 0.2167\n",
      "Epoch 180/100000\n",
      " - 1s - loss: 0.0052 - accuracy: 0.2202\n",
      "Epoch 181/100000\n",
      " - 1s - loss: 0.0032 - accuracy: 0.2405\n",
      "Epoch 182/100000\n",
      " - 1s - loss: 0.0033 - accuracy: 0.2473\n",
      "Epoch 183/100000\n",
      " - 1s - loss: 0.0056 - accuracy: 0.2138\n",
      "Epoch 184/100000\n",
      " - 1s - loss: 0.0036 - accuracy: 0.2319\n",
      "Epoch 185/100000\n",
      " - 1s - loss: 0.0039 - accuracy: 0.2331\n",
      "Epoch 186/100000\n",
      " - 1s - loss: 0.0032 - accuracy: 0.2380\n",
      "Epoch 187/100000\n",
      " - 1s - loss: 0.0047 - accuracy: 0.2234\n",
      "Epoch 188/100000\n",
      " - 1s - loss: 0.0047 - accuracy: 0.2286\n",
      "Epoch 189/100000\n",
      " - 1s - loss: 0.0036 - accuracy: 0.2427\n",
      "Epoch 190/100000\n",
      " - 1s - loss: 0.0031 - accuracy: 0.2483\n",
      "Epoch 191/100000\n",
      " - 1s - loss: 0.0029 - accuracy: 0.2603\n",
      "Epoch 192/100000\n",
      " - 1s - loss: 0.0056 - accuracy: 0.2220\n",
      "Epoch 193/100000\n"
     ]
    }
   ],
   "source": [
    "model.fit(trainx, trainy, epochs=100000, verbose=2)\n",
    "_, accuracy = model.evaluate(validx, validy)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(trainx, trainy, model):\n",
    "    mp.plot(model.predict(np.array([trainx]))[0])\n",
    "    mp.plot([trainx[i]/max(trainx) for i in range(bins - 1)])\n",
    "    mp.plot(trainy)\n",
    "    mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(trainx[300], trainy[300], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/oscar/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oscar/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      " - 1s - loss: 48.9602\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.5799\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.9263\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.8313\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7908\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.7537\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.7146\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6708\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6243\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5761\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5260\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.4741\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.4168\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.3580\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.3119\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2859\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2696\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2562\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2457\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2364\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2282\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.2211\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.2147\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.2080\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.2019\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1961\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1903\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1855\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1804\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1758\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1707\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1672\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1622\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1572\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.1532\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1498\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1457\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.1421\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1394\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1355\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1324\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.1305\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1267\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.1237\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1210\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.1187\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.1158\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.1135\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.1125\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.1113\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.1080\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.1053\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.1036\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.1020\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0999\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0977\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0959\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0943\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0928\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0912\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0898\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0889\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0882\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0862\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0844\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0824\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0815\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0803\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0795\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0777\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0773\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0754\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0746\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0737\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0729\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0715\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0710\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0701\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0695\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0681\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0669\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0665\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0653\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0643\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0636\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0634\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0632\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0621\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0613\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0605\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0613\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0590\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0582\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0569\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0561\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0554\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0552\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0546\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7efe4a623f28>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as mp\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "def plot(trainx, trainy, model):\n",
    "    mp.plot(model.predict(np.array([trainx]))[0])\n",
    "    mp.plot([trainx[i]/max(trainx) for i in range(bins - 1)])\n",
    "    mp.plot(trainy)\n",
    "    mp.show()\n",
    "    \n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "bins = 128\n",
    "seedmax = 20 # opens seed files 0 - 19. Lost too much data due to kernel crashes, so these got broken up\n",
    "trainx = []\n",
    "trainy = []\n",
    "validx = []\n",
    "validy = []\n",
    "\n",
    "#This is not a ... pythonic [barf]... way of reading data, but python is stupid about pointers, so deal with it\n",
    "for i in range(seedmax):\n",
    "    with open('test_pots'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            trainx.append([float(num) for num in row])\n",
    "    with open('test_out'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            trainy.append([float(num) for num in row])\n",
    "    with open('valid_pots'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            validx.append([float(num) for num in row])\n",
    "    with open('valid_out'+str(i)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            validy.append([float(num) for num in row])\n",
    "            \n",
    "model = Sequential()\n",
    "model.add(Dense(127, input_dim=127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.add(Dense(127, activation='softplus'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "trainx = np.array(trainx)\n",
    "trainy = np.array(trainy)\n",
    "validx = np.array(validx)\n",
    "validy = np.array(validy)\n",
    "\n",
    "model.fit(trainx, trainy, epochs=100, batch_size=960, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 68us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aa00dfc0025e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# serialize model to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(validx, validy)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
